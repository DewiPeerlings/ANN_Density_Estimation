{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import keras\n",
    "from keras import initializers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.constraints import max_norm\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import expon\n",
    "\n",
    "from time import time\n",
    "import math\n",
    "\n",
    "import random\n",
    "from scipy.misc import comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TUNING PARAMETERS ###\n",
    "\n",
    "observations = 1299                    # Sample construction\n",
    "X_val_size = 199\n",
    "n_split = 5\n",
    "                      \n",
    "N_nodes = [1, 2, 3]                    # Design MLP                       \n",
    "t=0\n",
    "H_layers = [1, 2, 3, 5, 10]\n",
    "\n",
    "\n",
    "m = 0                                  # Distribution parameters\n",
    "sd = 1               \n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "lambd = 1.5\n",
    "a = 0.25\n",
    "b = 0.75\n",
    "\n",
    "                                       # Grid search hyperparameters\n",
    "batch_size = [10, 25, 50, 100]\n",
    "epochs = [50, 100, 500]\n",
    "\n",
    "dropout_rate = [i/5 for i in range(0,5)]\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "weight_constraint = [0, 3, 5]\n",
    "\n",
    "neurons = [1, 2, 3, 4, 5, 10, 20, 30]\n",
    "\n",
    "                                       #grid_search or random_search?\n",
    "search = \"random_search\"\n",
    "n_iter_search = 10\n",
    "\n",
    "t_size = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### FUNCTIONS ###\n",
    "\n",
    "def create_dataset():\n",
    "    x = np.array([[random.expovariate(lambd) for i in range(N_nodes[t])] for i in range(0, observations+1)])\n",
    "    y = np.array([0.95*i/observations+0.05 for i in range(0,observations+1)])\n",
    "    \n",
    "    #check dimensionality\n",
    "    print('dimensionality of x :', np.shape(x))\n",
    "    print('dimensionality of y: ', np.shape(y))\n",
    "    \n",
    "    #sort\n",
    "    x_sorted = np.sort(x,0)\n",
    "\n",
    "    #correlation\n",
    "    for i in range(N_nodes[t]):\n",
    "        print(\"correlation for \" +str(N_nodes[t])+ \": \",pearsonr(x_sorted[:,0], y))\n",
    "    \n",
    "    print('\\n')\n",
    "    return (x_sorted,y)\n",
    "\n",
    "def scale(x):\n",
    "    ### scale ###\n",
    "    mean = np.mean(x, axis=0)\n",
    "    var = np.var(x, axis=0)\n",
    "    print(\"mean is: \", mean)\n",
    "    print(\"variance is: \", var)\n",
    "\n",
    "    x_scaled = np.array([(i-mean)/np.sqrt(var) for i in x])\n",
    "    \n",
    "    print('\\n')\n",
    "    return x_scaled\n",
    "\n",
    "\n",
    "def plot_dataset(x, y):\n",
    "    \n",
    "    for i in range(N_nodes[t]):\n",
    "        plt.scatter(x[:,i], y, c=y)\n",
    "        plt.ylabel('Cumulative Probability < RV value')\n",
    "        plt.xlabel('Random Variable (RV) value')\n",
    "        plt.plot(x[:,i], expon.cdf(x[:,i], scale=1/lambd), label=\"actual CDF\")\n",
    "        plt.colorbar(ticks=np.linspace(min(y), max(y), 10, endpoint=True))\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "def plot_cdf(X_train, Y_train, X_test, Y_test, X_train_scaled, X_test_scaled, predictions):\n",
    "        for i in range(N_nodes[t]):\n",
    "            plt.scatter(X_test[:,i], Y_test, label='test set')\n",
    "            plt.scatter(X_test[:,i], predictions, label='fitted set')\n",
    "            plt.scatter(X_test[:,i], expon.cdf(X_test[:,i],scale=1/lambd), label=\"actual CDF\")\n",
    "\n",
    "            plt.ylabel('Cumulative Probability < RV value')\n",
    "            plt.xlabel('Random Variable (RV) value')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "def create_model(H_layers=1, optimizer='Adam', init=initializers.RandomNormal(mean=0, stddev=0.25, seed=None), activation='sigmoid', dropout_rate=0.0, learn_rate=0.01, neurons=1, weight_constraint=0):\n",
    "#def create_model(optimizer, init, dropout_rate, learn_rate, activation, neurons, weight_constraint):\n",
    "    ### CREATE MODEL ###\n",
    "    MLP = Sequential()\n",
    "    #MLP.add(Dropout(dropout_rate, input_shape=(N_nodes[t],)))\n",
    "    MLP.add(Dense(neurons, input_dim=N_nodes[t], kernel_initializer=init, activation=activation, kernel_constraint=max_norm(weight_constraint)))    #hidden layer\n",
    "    #MLP.add(Dropout(dropout_rate))\n",
    "    for h in range(1,H_layers):                                                                     #multiple hidden layers\n",
    "        MLP.add(Dense(neurons, kernel_initializer=init, activation=activation, kernel_constraint=max_norm(weight_constraint)))\n",
    "        #MLP.add(Dropout(dropout_rate))\n",
    "    MLP.add(Dense(1, kernel_initializer=init, activation='sigmoid'))                                 #output layer\n",
    "    MLP.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return MLP\n",
    "\n",
    "def train_ANN(search, X_train, Y_train, X_test):\n",
    "    \n",
    "    model = keras.wrappers.scikit_learn.KerasRegressor(build_fn=create_model, verbose=0)\n",
    "    print('Keras wrapper done')\n",
    "    param = dict(epochs=epochs, batch_size=batch_size, neurons=neurons, H_layers=H_layers, learn_rate=learn_rate, weight_constraint=weight_constraint)\n",
    "    #param = dict(neurons=neurons, H_layers=H_layers, learn_rate=learn_rate, weight_constraint=weight_constraint)\n",
    "    \n",
    "    if (search == 'random_search'):\n",
    "        print('RANDOM SEARCH')\n",
    "        grid = RandomizedSearchCV(estimator=model, param_distributions=param, n_iter=n_iter_search, n_jobs=1)\n",
    "        print('Grid is prepared')\n",
    "        start = time()\n",
    "        grid.fit(X_train, Y_train)\n",
    "        #grid.fit(X_train, Y_train, batch_size=500, epochs=1000)\n",
    "        print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "              \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "    else:\n",
    "        print('GRIDSEARCH')\n",
    "        grid = GridSearchCV(estimator=model, param_grid=param, n_jobs=1)\n",
    "        print('Grid is prepared')\n",
    "        start = time()\n",
    "        grid.fit(X_train, Y_train)\n",
    "        #grid.fit(X_train, Y_train, batch_size=500, epochs=1000)\n",
    "        print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "              % (time() - start, len(grid.cv_results_['params'])))\n",
    "\n",
    "    # summarize results (smallest loss?)\n",
    "    print(\"Best: %f using %s\" % (-grid.best_score_, grid.best_params_))\n",
    "    means = grid.cv_results_['mean_test_score']\n",
    "    stds = grid.cv_results_['std_test_score']\n",
    "    params = grid.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (-mean, stdev, param))\n",
    "    \n",
    "    predictions = grid.best_estimator_.predict(X_test)\n",
    "    \n",
    "    print('\\n')\n",
    "    return (grid, predictions)\n",
    "\n",
    "def run_ANN(X_train, Y_train, X_test):\n",
    "    \n",
    "    model = create_model(H_layers=H_layers, learn_rate=learn_rate, neurons=neurons, weight_constraint=weight_constraint)\n",
    "    print('Keras wrapper done')\n",
    "    model.compile(loss='mean_squared_error', optimizer='Adam', metrics=['mae'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    print('\\n')\n",
    "    return (model, predictions)\n",
    "\n",
    "def activation_function(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "### CHECK PROCEDURE ON TEST SET ###\n",
    "\n",
    "def run_ANN_manually(X_val, X_val_scaled):\n",
    "    #assuming biases are included\n",
    "    b = 0\n",
    "    k = 0\n",
    "    #print('k', k)\n",
    "    temp_saved = np.zeros([len(X_val_scaled), H_layers+1, neurons])\n",
    "    predictions = np.zeros([len(X_val_scaled)])\n",
    "    #print('temp_saved', temp_saved)\n",
    "    while (H_layers-k)>=0:\n",
    "        #print('while (H_layers-k)>=0:' ,H_layers-k)\n",
    "        if (k==0):\n",
    "            #print('k==0')\n",
    "            for i in range(len(X_val_scaled)):\n",
    "                #print('i', i)\n",
    "                for j in range(neurons):\n",
    "                    #print('j', j)\n",
    "                    for l in range(N_nodes[t]):\n",
    "                        #print('l', l)\n",
    "                        temp_saved[i][k][j] += X_val_scaled[i][l]*weights[b][l][j]\n",
    "                        #print('temp_saved', temp_saved)\n",
    "                    temp_saved[i][k][j] += 1*weights[b+1][j]\n",
    "                    #print('temp_saved', temp_saved)\n",
    "                    temp_saved[i][k][j] = activation_function(temp_saved[i][k][j])\n",
    "            #print('temp_saved', temp_saved)\n",
    "            b += 2\n",
    "            k += 1\n",
    "        elif((H_layers-k)>0 and k!=0):\n",
    "            #print('(H_layers-k)>0 and k!=0')\n",
    "            #print('k', k)\n",
    "            for i in range(len(X_val_scaled)):\n",
    "                #print('i', i)\n",
    "                for j in range(neurons):\n",
    "                    #print('j', j)\n",
    "                    for l in range(neurons):\n",
    "                        #print('l', l)\n",
    "                        temp_saved[i][k][j] += temp_saved[i][k-1][l]*weights[b][l][j]\n",
    "                        #print('temp_saved', temp_saved)\n",
    "                    temp_saved[i][k][j] += 1*weights[b+1][j]\n",
    "                    #print('1*weights[k+2][j]', 1*weights[k+2][j])\n",
    "                    #print('temp_saved', temp_saved)\n",
    "                    temp_saved[i][k][j] = activation_function(temp_saved[i][k][j])\n",
    "            #print('temp_saved', temp_saved)\n",
    "            b += 2\n",
    "            k += 1\n",
    "        elif(H_layers-k)==0:\n",
    "            #print('(H_layers-k)==0')\n",
    "            #print('k', k)\n",
    "            for i in range(len(X_val_scaled)):\n",
    "                #print('i', i)\n",
    "                for l in range(neurons):\n",
    "                    #print('l', l)\n",
    "                    temp_saved[i][k][0] += temp_saved[i][k-1][l]*weights[b][l][0]\n",
    "                    predictions[i] += temp_saved[i][k-1][l]*weights[b][l][0]\n",
    "                    #print('temp_saved', temp_saved)\n",
    "                temp_saved[i][k][0] += 1*weights[b+1][0]\n",
    "                predictions[i] += 1*weights[b+1][0]\n",
    "                #print('temp_saved', temp_saved)\n",
    "                temp_saved[i][k][0] = activation_function(temp_saved[i][k][0])\n",
    "                predictions[i] = activation_function(predictions[i])\n",
    "            #print('temp_saved', temp_saved)\n",
    "            k += 1                                                     #stop while-loop\n",
    "\n",
    "    plot_cdf(X_test, Y_test, X_val, Y_val, X_test_scaled, X_val_scaled, predictions)\n",
    "\n",
    "    return (temp_saved, predictions)\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))*(1-1/(1+math.exp(-x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### K-FOLD SIMULATIONS ###\n",
    "\n",
    "best_score = np.empty([n_split,])\n",
    "dict_H_layers = np.empty([n_split,])\n",
    "dict_learn_rate = np.empty([n_split,])\n",
    "dict_neurons = np.empty([n_split,])\n",
    "dict_weight_constraint = np.empty([n_split,])\n",
    "dict_epochs = np.empty([n_split,])\n",
    "dict_batch_size = np.empty([n_split,])\n",
    "all_X_test_scaled = np.empty([n_split,int((observations+1-X_val_size-1)/n_split)])\n",
    "all_predictions = np.empty([n_split,int((observations+1-X_val_size-1)/n_split)])\n",
    "\n",
    "#create dataset\n",
    "x, y = create_dataset()\n",
    "plot_dataset(x, y)\n",
    "\n",
    "#take validation set out\n",
    "\n",
    "#X_val = x[0:(X_val_size+1)]\n",
    "#x = x[(X_val_size+1):(observations+1)]\n",
    "#Y_val = y[0:(X_val_size+1)]\n",
    "#y = y[(X_val_size+1):(observations+1)]\n",
    "\n",
    "#index = [random.randint(0,(observations+1)) for i in range(X_val_size+1)]\n",
    "index = np.random.choice(observations+1, X_val_size+1, replace=False)\n",
    "X_val = x[index]\n",
    "Y_val = y[index]\n",
    "\n",
    "x = np.delete(x, index, 0)\n",
    "y = np.delete(y, index, 0)\n",
    "\n",
    "index_sort = np.argsort(X_val[:,0])\n",
    "X_val = X_val[index_sort]\n",
    "Y_val = Y_val[index_sort]\n",
    "\n",
    "print('dimensionality of X_val: \\t', np.shape(X_val))\n",
    "print('dimensionality of Y_val: \\t', np.shape(Y_val))\n",
    "print('dimensionality of x : \\t\\t', np.shape(x))\n",
    "print('dimensionality of y: \\t\\t', np.shape(y), '\\n')\n",
    "\n",
    "kf = KFold(n_splits=n_split, shuffle=True, random_state=None)\n",
    "kf.get_n_splits(x)\n",
    "k = 1\n",
    "\n",
    "print('Simulations')\n",
    "M = 0\n",
    "for train_index, test_index in kf.split(x):\n",
    "    \n",
    "    #create k-fold sets\n",
    "    print(k, ':', 'TRAIN: ', len(train_index), 'TEST: ', len(test_index))\n",
    "    #print('TRAIN: ', train_index, 'TEST: ', test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    Y_train, Y_test = y[train_index], y[test_index]\n",
    "    print('X_train sample:', len(X_train), 'Y_train sample:', len(Y_train))\n",
    "    print('first entry of sample X_test', X_test[0], 'first entry of sample Y_test', Y_test[0], '\\n')\n",
    "\n",
    "    print('X_train scaling:')\n",
    "    X_train_scaled = scale(X_train)\n",
    "    print('X_test scaling:')\n",
    "    X_test_scaled = scale(X_test)\n",
    "\n",
    "    MLP = create_model()\n",
    "    print('MLP created')\n",
    "    grid, predictions = train_ANN(search, X_train_scaled, Y_train, X_test_scaled)\n",
    "    plot_cdf(X_train, Y_train, X_test, Y_test, X_train_scaled, X_test_scaled, predictions)\n",
    "    ###\n",
    "\n",
    "    best_score[M] = -grid.best_score_\n",
    "    dict_H_layers[M] = grid.best_params_['H_layers']\n",
    "    dict_learn_rate[M] = grid.best_params_['learn_rate']\n",
    "    dict_neurons[M] = grid.best_params_['neurons']\n",
    "    dict_weight_constraint[M] = grid.best_params_['weight_constraint']\n",
    "    dict_epochs[M] = grid.best_params_['epochs']\n",
    "    dict_batch_size[M] = grid.best_params_['batch_size']\n",
    "    all_predictions[M] = predictions\n",
    "    all_X_test_scaled[M] = X_test_scaled[:,0]\n",
    "    \n",
    "    k += 1\n",
    "    M += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#choose structure for MLP\n",
    "print(\"Best error score: \", np.amin(best_score), \"\\n\")\n",
    "print(\"Chosen structure: \", '{ weight_constraint: ',dict_weight_constraint[np.argmin(best_score)],', neurons: ', dict_neurons[np.argmin(best_score)], ', learn_rate', dict_learn_rate[np.argmin(best_score)], ', epochs: ', dict_epochs[np.argmin(best_score)], ', batch_size', dict_batch_size[np.argmin(best_score)], ', H_layers', dict_H_layers[np.argmin(best_score)],'}' )\n",
    "\n",
    "# fix hyperparameters\n",
    "weight_constraint = int(dict_weight_constraint[np.argmin(best_score)])\n",
    "neurons = int(dict_neurons[np.argmin(best_score)])\n",
    "learn_rate = dict_learn_rate[np.argmin(best_score)]\n",
    "epochs = int(dict_epochs[np.argmin(best_score)])\n",
    "batch_size = int(dict_batch_size[np.argmin(best_score)])\n",
    "H_layers = int(dict_H_layers[np.argmin(best_score)])\n",
    "\n",
    "#run on last test set ONLY (?)\n",
    "model, predictions = run_ANN(X_train_scaled, Y_train, X_test_scaled)\n",
    "plot_cdf(X_train, Y_train, X_test, Y_test, X_train_scaled, X_test_scaled, predictions)\n",
    "\n",
    "#retrieve these weights\n",
    "#(map weights MLP to validation set)\n",
    "weights = model.get_weights()\n",
    "print(\"length weights\", len(weights))\n",
    "for i in range(len(weights)):\n",
    "    print(\"layers length: (weight + possible bias)\", weights[i].size)\n",
    "\n",
    "#print(model.get_weights())\n",
    "#print(model.get_config())\n",
    "\n",
    "### CHECK PROCEDURE ON TEST SET ###\n",
    "temp_not_saved, Tpredictions = run_ANN_manually(X_test, X_test_scaled)\n",
    "### WARNING (still small differences)\n",
    "np.in1d(predictions,Tpredictions)\n",
    "diff = predictions[:,0]-Tpredictions\n",
    "print('differences between keras and self-constructed-procedure: \\n', abs(diff))\n",
    "\n",
    "### MAP TESTWEIGHTS ON VALSET ###\n",
    "print('X_val scaling:')\n",
    "X_val_scaled = scale(X_val)\n",
    "temp_saved, predictions = run_ANN_manually(X_val, X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def derivative(): \n",
    "    #i = len(temp_saved[0])-1\n",
    "    i = 0\n",
    "    k = len(weights)-4\n",
    "    print('iterator temp_saved i: ', i, 'iterator weights k: ', k)\n",
    "\n",
    "    #neurons last hidden layer\n",
    "    #temp_first = np.zeros([len(predictions), neurons])\n",
    "    #for obs in range(len(predictions)):\n",
    "        #print('obs', obs)\n",
    "    #    for j in range(neurons):\n",
    "            #print('j', j)\n",
    "    #        temp_first[obs][j] = weights[k][j][0]\n",
    "    #print('temp_first', temp_first)\n",
    "    #k -= 2\n",
    "    #i -= 1\n",
    "    #i += 1\n",
    "    #print('temp_first (neurons last hidden layer)', temp_first[0], '\\n')\n",
    "    #print('iterator temp_saved i: ', i, 'iterator weights k: ', k)\n",
    "\n",
    "    #temp_layers (neurons middle layers)\n",
    "    if H_layers > 1:\n",
    "        print('STEP LAYERS')\n",
    "        temp_layers = dict()\n",
    "        qq = H_layers-1\n",
    "        for q in range(0, (H_layers-1)):\n",
    "            print('NEW LAYER')\n",
    "            print('q', q)\n",
    "            temp_layers[q] = np.zeros([len(predictions), neurons])\n",
    "            for obs in range(len(predictions)):\n",
    "                #print('NEW OBS')\n",
    "                #print('obs', obs)\n",
    "                for p in range(neurons):\n",
    "                    #print('p', p)\n",
    "                    if(q==0):\n",
    "                        #print('IF')\n",
    "                        for l in range(neurons):\n",
    "                            #print('l', l)\n",
    "                            temp_sum = 0\n",
    "                            for m in range(neurons):\n",
    "                                #print('m', m)\n",
    "                                #print('weights[k][m][l]','weights[',k,'][',m,'][',l,']')\n",
    "                                #print('temp_saved[obs][qq-1][m]','temp_saved[',obs,'][',qq-1,'][',m,']')\n",
    "                                temp_sum += temp_saved[obs][qq-1][m]*weights[k][m][l]\n",
    "                            #print('derivative_sigmoid(temp_sum+weights[k+1][p])','derivative_sigmoid(temp_sum+weights[',k+1,'][',l,'])')\n",
    "                            #print('weights[k+2][l][0]','weights[',k+2,'][',l,'][',0,']')\n",
    "                            #print('weights[k][p][l]','weights[',k,'][',p,'][',l,']')\n",
    "                            temp_layers[q][obs][p] += weights[k+2][l][0]*derivative_sigmoid(temp_sum+weights[k+1][l])*weights[k][p][l]\n",
    "                    else:\n",
    "                        #print('ELSE')\n",
    "                        for l in range(neurons):\n",
    "                            #print('l', l)\n",
    "                            temp_sum = 0\n",
    "                            for m in range(neurons):\n",
    "                                #print('m', m)\n",
    "                                #print('weights[k][m][l]','weights[',k,'][',m,'][',l,']')\n",
    "                                #print('temp_saved[obs][qq-1][m]','temp_saved[',obs,'][',qq-1,'][',m,']')\n",
    "                                temp_sum += temp_saved[obs][qq-1][m]*weights[k][m][l]\n",
    "                            #print('derivative_sigmoid(temp_sum+weights[k+1][l])','derivative_sigmoid(temp_sum+weights[',k+1,'][',l,'])')\n",
    "                            #print('temp_layers[q-1][obs][l]','temp_layers[',q-1,'][',obs,'][',l,']')\n",
    "                            #print('weights[k][p][l]','weights[',k,'][',p,'][',l,']')\n",
    "                            temp_layers[q][obs][p] += temp_layers[q-1][obs][l]*derivative_sigmoid(temp_sum+weights[k+1][l])*weights[k][p][l]\n",
    "\n",
    "            k -= 2\n",
    "            i += 1\n",
    "            qq -= 1\n",
    "\n",
    "    print('iterator temp_saved i: ', i, 'iterator weights k: ', k)\n",
    "\n",
    "    #derivative input layer\n",
    "    print('LAST STEP')\n",
    "    temp_last = np.zeros([len(predictions), N_nodes[t]])\n",
    "    for obs in range(len(predictions)):\n",
    "        #print('NEW OBS')\n",
    "        #print('obs', obs)\n",
    "        for pp in range(N_nodes[t]):\n",
    "            #print('pp', pp)\n",
    "            for p in range(neurons):\n",
    "                #print('p', p)\n",
    "                if H_layers == 1:\n",
    "                    #print('IF')\n",
    "                    temp_sum = 0\n",
    "                    #for l in range(N_nodes[t]):\n",
    "                        #print('l', l)\n",
    "                    for m in range(N_nodes[t]):\n",
    "                        #print('m', m)\n",
    "                        #print('weights[0][m][p]', 'weights[0][',m,'][',p,']')\n",
    "                        #print('X_val[obs][m]', 'X_val[',obs,'][',m,']')\n",
    "                        temp_sum += weights[0][m][p]*X_val[obs][m]\n",
    "                    #print('temp_sum+weights[k+1][p]', 'temp_sum+weights[',k+1,'][',p,']')\n",
    "                    #print('weights[len(weights)-2][p][0]', 'weights[',len(weights)-2,'][',p,'][0]')\n",
    "                    #print('weights[0][0][p]', 'weights[0][0][',p,']')\n",
    "\n",
    "                    #temp_last[obs][pp] += temp_first[obs][p]*derivative_sigmoid(temp_sum+weights[k+1][p])*weights[0][0][p]\n",
    "                    #print('previous temp_last[obs][pp]', temp_last[obs][pp])\n",
    "                    temp_last[obs][pp] += weights[len(weights)-2][p][0]*derivative_sigmoid(temp_sum+weights[k+1][p])*weights[0][0][p]\n",
    "                    #print('temp_last[obs][pp]', 'temp_last[',obs,'][',pp,']', weights[len(weights)-2][p][0]*derivative_sigmoid(temp_sum+weights[k+1][p])*weights[0][0][p])\n",
    "                    #print('temp_last[obs]', temp_last[obs])\n",
    "                    \n",
    "                else:\n",
    "                    #print('ELSE')\n",
    "                    temp_sum = 0\n",
    "                    for l in range(N_nodes[t]):\n",
    "                        #print('l', l)\n",
    "                        for m in range(N_nodes[t]):\n",
    "                            #print('m', m)\n",
    "                            #print('weights[0][m][p]', 'weights[0][',m,'][',p,']')\n",
    "                            #print('X_val[obs][m]', 'X_val[',obs,'][',m,']')\n",
    "                            temp_sum += weights[0][m][p]*X_val[obs][m]\n",
    "                        #print('temp_sum+weights[k+1][p]', 'temp_sum+weights[',k+1,'][',p,']')\n",
    "                        #print('temp_layers[(H_layers-2)][obs][p]', 'temp_layers[(',H_layers-2,')][',obs,'][',p,']')\n",
    "                        #print('weights[0][0][p]', 'weights[0][0][',p,']')\n",
    "\n",
    "                        #temp_last[obs][pp] += temp_layers[(H_layers-2)][obs][p]*derivative_sigmoid(temp_sum+weights[k+1][p])*weights[0][0][p]\n",
    "                        temp_last[obs][pp] += temp_layers[len(temp_layers.keys())-1][obs][p]*derivative_sigmoid(temp_sum+weights[k+1][p])*weights[0][l][p]\n",
    "\n",
    "    return temp_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### DERIVATIVE H=? ###\n",
    "outcome = derivative()\n",
    "\n",
    "for i in range(N_nodes[t]):\n",
    "    #plt.scatter(X_val_scaled, temp_last[:,0], label=\"scaled fitted set\")\n",
    "    #plt.scatter(X_val_scaled, expon.pdf(X_test,scale=1/lambd), label=\"scaled actual PDF\")\n",
    "    plt.scatter(X_val[:,i], outcome[:,i], label=\"fitted set\")\n",
    "    plt.scatter(X_val[:,i], expon.pdf(X_test[:,i],scale=1/lambd), label=\"actual PDF\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for i in range(N_nodes[t]):\n",
    "    plt.scatter(X_val[:,i], outcome[:,i], label=\"fitted set\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
